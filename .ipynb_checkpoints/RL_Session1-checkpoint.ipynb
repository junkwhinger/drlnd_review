{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맨땅에 강화학습 - 1\n",
    "\n",
    "수학의 정석을 펼치면 행렬이 나오듯 강화학습 책을 열면 먼저 MDP(Markov Decision Process)가 기다리고 있다.\n",
    "\n",
    "<img src='assets/AAMarkov.jpg' width=200px>\n",
    "*러시아의 수학자 안드레 안드레비치 마르코프*\n",
    "\n",
    "강화학습 공부를 여러번 시도했지만 MDP가 뭐지? -> 마르코프 프로세스가 뭐지? -> 전이확률이 뭐지? -> 다음에 알아보자..\n",
    "의 과정을 거친 어두운 과거가 있었다.\n",
    "\n",
    "이번에는 이론보다 먼저 문제부터 접근해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI GYM\n",
    "\n",
    "그동안 공부해봤던 딥러닝은 인풋-타겟 데이터를 준비하고 이를 사용해 네트워크를 학습시키기만 하면 되었다.  \n",
    "\n",
    "강화학습은 조금 다르다. 학습 시키는 것은 같지만, 인풋과 타겟을 준비해두는 개념은 아니다.  \n",
    "\n",
    "예를 들어 미로를 탈출하는 봇을 학습시킨다고 하면, 미로 환경을 먼저 준비해야 한다.   \n",
    "\n",
    "그 미로 환경안에서 우리의 봇은 실패와 성공을 거듭하며 탈출하는 방법을 익히게 된다.  \n",
    "\n",
    "즉, 강화학습을 하려면 환경과 에이전트(봇)을 준비해야 하는데 이 분야에서 가장 많이 활용되는 라이브러리는 OpenAI에서 제공하는 `gym`이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gym`은 여러 편리한 환경들을 미리 만들어두었다. 우리는 그것을 가져다 쓰기만 하면 된다.  \n",
    "\n",
    "처음 배우자마자 스타크래프트 AI나 퀘이크 AI를 만들 수는 없다.  \n",
    "\n",
    "가장 만만해보이는 녀석을 하나 골라보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py\">`CliffWalking`</a> 문제는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "cliff_env = np.ones(env.shape)\n",
    "cliff_env[3, :] = -1\n",
    "cliff_env[3, 0] = 0\n",
    "cliff_env[3, -1] = 0\n",
    "fig, ax = plt.subplots(figsize=env.shape[::-1])\n",
    "sns.heatmap(cliff_env, linewidths=1, ax=ax, cmap='RdBu')\n",
    "\n",
    "ax.text(0.2, 3.5, \"START\")\n",
    "ax.text(11.3, 3.5, \"END\")\n",
    "\n",
    "for i in range(10):\n",
    "    ax.text(i+1 + 0.2, 3.5, \"CLIFF\", color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주인공 에이전트의 미션은 좌하단 스타팅에서 우하단 도착지까지 이동하는 것이다.  \n",
    "\n",
    "에이전트는 위, 아래, 왼쪽, 오른쪽으로 1칸씩 이동할 수 있으며 이동할때마다 -1점을 받는다.\n",
    "\n",
    "만약 CLIFF로 표시된 셀로 이동하게 되면 -100점을 받으며 에이전트는 낙사하여 게임을 다시 시작하지.\n",
    "\n",
    "이 게임의 목적은 에이전트가 살아있는 채로 스타팅에서 도착지로 가장 최적으로 도달하는 방법을 학습하는 것이다.\n",
    "\n",
    "`env` 변수에 할당한 환경에서 몇가지 정보를 뽑아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경의 크기\n",
    "env.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에이전트가 취할 수 있는 가짓수\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스타팅 셀 (36번째 셀)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 우리한테 쉽다고 쉬운 문제는 아니다.\n",
    "\n",
    "이 문제를 우리가 푸는데 0.1초도 필요하지 않다. 그냥 눈으로 봐도 답은 정해져있다.  \n",
    "\n",
    "위로 1칸만 올라간다음, 11칸을 직진하고 다시 1칸을 내려오면 된다. \n",
    "\n",
    "<img src='assets/cliff_human_approach.png' width=400px>\n",
    "\n",
    "개/고양이 분류하는 딥러닝처럼 사람에게 쉬운 문제가 컴퓨터에게는 매우 어려울 수 있다.\n",
    "\n",
    "우리의 agent도 마찬가지다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그럼 어떻게 풀어야 할까?\n",
    "\n",
    "이 문제에서 agent은 매 순간 의사결정을 내려야한다. 상하좌우 4가지 action 중에 하나를 골라야 한다.  \n",
    "\n",
    "그리고 그 선택에 따라 다음 셀로 이동한 봇은 또다시 선택에 직면한다. 새로운 state에서.\n",
    "\n",
    "스타팅에 agent 대신 우리가 서서 주변을 둘러본다고 생각해보자. \n",
    "\n",
    "위에는 평지가 있고 왼편에는 깎아지듯 떨어지는 절벽이 있다. 그리고 절벽의 먼 너머에는 도착지가 있다.\n",
    "\n",
    "절벽으로 발을 내딫으면 도착지에 더 가까워지지만 목숨이 날아간다. 평지로 이동하면 거리는 줄어들지 않지만 그래도 목숨은 부지한다.\n",
    "\n",
    "우리 상식으로는 후자가 전자보다 더 value있는 행위이므로 (죽으면 말짱 소용없으니) 평지로 이동하는 것을 택한다.\n",
    "\n",
    "이렇듯 우리는 다음 action을 선택할 때 가장 value가 높은 쪽을 선택한다. \n",
    "\n",
    "<img src='assets/cliff_first_move.jpg' width=400px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent를 움직여보자.\n",
    "\n",
    "agent는 어떻게 env (게임 환경)과 상호작용할까?  \n",
    "\n",
    "Cliff Walking 문제에서 가장 처음 시작하는 지점은 36번째 셀이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스타팅 state\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent가 취할 수 있는 action은 0, 1, 2, 3이며 각 숫자는 다음의 방향으로의 1칸 이동을 의미한다.\n",
    "\n",
    "- UP = 0\n",
    "- RIGHT = 1\n",
    "- DOWN = 2\n",
    "- LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2action = {\n",
    "    0:'UP',\n",
    "    1:'RIGHT',\n",
    "    2:'DOWN',\n",
    "    3:'LEFT'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위로 이동해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "env의 `step`함수에 action을 인자로 입력하면, env내의 agent의 위치정보가 갱신되고, 이에 해당하는 4가지 정보가 리턴된다.\n",
    "\n",
    "- observation(object): action으로 인해 발생하는 관측값으로, 여기서는 다음 시점의 state가 된다.\n",
    "- reward(float): action으로 인해 얻게 되는 reward값. 강화학습의 목적은 총 reward (value)를 최대화시키는 것이 된다.\n",
    "- done(boolean): env를 리셋해야하는지에 대한 불리언 값. 강화학습이 episodic task인 경우 done은 episode의 종료를 의미한다. (후에 설명)\n",
    "- info(dict): 디버깅을 위한 정보로 마지막 state 변화에 대한 raw 확률값을 담는다. 학습에는 사용하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent는 env안에서 state를 파악하고, action을 실행하여, action에 대한 reward와 다음 state 정보를 얻는다.\n",
    "\n",
    "강화학습에서는 state 파악과 action 실행을 같은 시점으로, reward와 다음 state 정보 획득을 같은 시점으로 묶는다.\n",
    "\n",
    "예를 들어 현재 시점을 t라고 하면,\n",
    "\n",
    "- agent는 env로부터 현재 state 정보 $S_t$를 얻는다.\n",
    "- agent는 일련의 판단을 통해 action $A_t$를 선택하고 실행한다.\n",
    "- env는 agent의 action에 따라 reward $R_{t+1}$을 전달한다.\n",
    "- agent의 action에 따라 새로운 state 정보 $S_{t+1}$이 agent에게 전달된다.\n",
    "\n",
    "\n",
    "<img src='assets/env_agent_interaction.jpg' width=400px>\n",
    "\n",
    "env가 리셋된 후 첫 스타팅에서는 리워드 정보가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### value는 어떻게 계산해야 할까?\n",
    "\n",
    "이처럼 agent는 현재 state에서 action을 수행한다음 env로부터 reward와 새로운 state 정보를 얻는다.\n",
    "\n",
    "문제를 풀기 위해 agent는 reward의 총합, 즉 value가 가장 큰 쪽으로 움직이도록 학습한다.\n",
    "\n",
    "그렇다면 value라는 것은 어떻게 계산해야 할까??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='assets/first_move.jpg' width=200px>\n",
    "\n",
    "스타팅에서 갈 수 있는 다음 state는 바로 위와 오른쪽이었다.   \n",
    "\n",
    "agent는 위 state의 value가 오른쪽 절벽의 value보다 더 높다고 판단한다.   \n",
    "\n",
    "절벽으로 이동할 때 agent는 -100을, 평지로 이동할때는 -1의 reward를 environment로부터 받는다.  \n",
    "\n",
    "그렇다면 스타팅 state에서 이동하는 경우 위 state는 -1, 오른쪽 state는 -100의 value를 가지고 있다고 생각할 수 있다.\n",
    "\n",
    "-1 > -100이므로 바로 다음 reward에 기반한 value 책정은 나쁘지 않은 방법이다.\n",
    "\n",
    "하지만 코앞의 reward만으로 우리는 의사결정을 내리지 않는다. \n",
    "\n",
    "연속적으로 action을 결정해야 하는 상황이라면, 또 지금의 action이 나중의 action에도 영향을 준다면,  \n",
    "\n",
    "지금 시점의 value는 앞으로 받게 될 reward들의 총합($G_t$)이라고 생각하는 것이 옳다.\n",
    "\n",
    "$G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...$\n",
    "\n",
    "그런데 시점이 다른 두 reward의 크기가 같다해도, 언제 얻을 수 있느냐에 따라 그 가치가 달라질 수 있다.  \n",
    "\n",
    "당장 받는 1달러는 내일 받는 1달러보다 가치가 조금 더 높다. 따라서 미래 가치에 대한 discount rate인 $\\gamma$를 적용한다. \n",
    "\n",
    "discount rate를 적용된 value의 식은 아래와 같다.\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...$\n",
    "\n",
    "$\\gamma$는 다음과 같은 특성을 지닌다.\n",
    "\n",
    "- $0<= \\gamma <= 1$ discount rate는 미래 가치를 조금 깎으므로 0과 1사이에 위치한 어떤 값이 된다.\n",
    "- $\\gamma$가 0이 되면 $G_t = R_{t+1}$가 되므로, agent는 바로 다음의 reward만을 신경쓰게 된다.\n",
    "- $\\gamma$가 1이 되면 $G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...$가 되므로, agent는 먼 미래의 reward도 코앞의 reward만큼 중요하게 신경쓴다.\n",
    "- 즉, $\\gamma$가 커지면 커질수록 미래의 reward를 더 신경쓴다고 볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Step Dynamics\n",
    "\n",
    "정리하자면, agent는 현재 state에서 실행할 action을 결정해야 한다. \n",
    "\n",
    "어떤 action을 선택할까? 돌아오는 return $G_t$가 가장 높은 action을 선택한다.  \n",
    "\n",
    "앞에서 $G_t$는 당장의 reward 뿐 아니라 먼 미래의 값까지 할인해서 더한 값이라고 정의했다.\n",
    "\n",
    "즉, agent는 이런 복잡한 연속적인 의사결정의 문제를\n",
    "\n",
    "지금 이 state에서 할 수 있는 action 중에 value가 가장 높은 action은 뭐지?라는 작은 문제의 연속으로 만들어서 푼다.\n",
    "\n",
    "이를 one step dynamics라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite MDPs\n",
    "\n",
    "지금까지 설명한 내용이 강화학습의 기본이다.\n",
    "\n",
    "우리의 agent는 절벽을 무사히 그리고 빠르게 건너는 문제를 푼다. 이 agent가 어떤 action을 선택하느냐에 따라 reward가 달라지고 (-1 or -100), 다음 state가 달라진다. agent는 변화하는 환경과 상호작용하면서 문제를 푸는 방식을 익힌다. 이것이 reinforcement learning, 강화학습이다.\n",
    "\n",
    "agent는 action을 선택함에 있어 value 최대화를 기준으로 삼는다. value는 단기적인 reward 뿐 아니라 먼 미래의 reward도 discount하여 고려한다. 즉, agent는 expected cumulative reward를 최대화하며 문제를 푸는 것을 목표로 학습한다.\n",
    "\n",
    "agent가 활동하는 environment는 one-step dynamics라는 특징을 가진다. t-1시점의 정보는 t시점으로, t시점의 정보는 t+1시점으로 사슬고리가 연결되듯 environment가 구성되어있다.\n",
    "\n",
    "이것이 바로 MDP의 속성이다. Cliff Walking 문제의 state와 action은 특히 그 경우의 수가 한정되어 있으므로 Finite MDP라고 할 수 있다. \n",
    "\n",
    "Finite MDPs는 다음과 같은 특징을 가진다.\n",
    "\n",
    "- $S$ = a finite set of states\n",
    "- $S^+$ = a finite set of states (episodic task)\n",
    "- $A$ = a finite set of actions\n",
    "- $R$ = a set of rewards\n",
    "- one-step dynamics\n",
    "- $\\gamma$ = the discount rate\n",
    "\n",
    "문제가 시작과 끝이 있으면 episodic task, 끝이 없이 계속 되는 문제를 continuous task라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "\n",
    "강화학습을 푼다는 것은, 연속적으로 의사결정을 내려야 하는 상황에서 최적의 의사결정을 내리는 추상적인 전략을 깨우친다는 것과 같다.\n",
    "\n",
    "서점에 가보면 엄청나게 많은 주식투자 책을 찾아볼 수 있다. 단기적인 시장 상황을 통해 매매하는 기술적 투자나 회사의 내재적인 가치를 판단해 저평가 주식을 찾아내는 가치 투자 등등 저마다 다양한 투자 정책을 제시한다.\n",
    "\n",
    "Cliff Walking로 마찬가지다. 시작점에서 출발한 agent가 종료지점까지 도착하는데는 다양한 길이 있다. 절벽을 따라 걸을 수도 있고, 절벽에서 멀리 떨어져서 걸을 수도 있다.\n",
    "\n",
    "policy(정책)은 $\\pi$라는 함수로 생각하면 된다.   \n",
    "이 함수에 state $s$를 입력하면 그 policy를 따르는 action $a$를 얻을 수 있다. \n",
    "\n",
    "이를 수식으로 표현하면, \n",
    "$\\pi : S \\rightarrow A$라 할 수 있다.\n",
    "\n",
    "$s$를 넣으면 무조건 policy에 따른 $a$가 100% 출력되는 policy를 deterministic policy라고 한다.\n",
    "\n",
    "그런데 우리 인생이 항상 의도한대로 흘러가지 않듯, 오른쪽으로 가려는 agent도 강한 바람에 위로 이동할 수도 있다.\n",
    "\n",
    "이렇게 확률적 요소가 반영된 policy를 stochastic policy라고 한다. \n",
    "\n",
    "이 함수는 state $s$에서 action $a$를 수행할 확률(0에서 1사이)을 리턴한다.\n",
    "\n",
    "즉, 수식으로 표현하면\n",
    "$\\pi : S \\times A \\rightarrow [0, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy 구현\n",
    "\n",
    "Cliff Walking에서 각 state는 4가지 action을 수행할 수 있다. 어떤 state s에서의 policy를 각 스테이트가 가질 수 있는 4가지 액션값이 발생할 수 있는 확률 값을 담은 array라고 생각해보자.\n",
    "\n",
    "두가지 policy를 구현한 후 각 policy의 value를 살펴보자.\n",
    "\n",
    "- random_policy: 어떤 state에 있든 4가지 액션 중 하나를 랜덤하게 선택한다. (각 25%의 확률)\n",
    "- optimal_policy: 위로 한칸 이동한 후 끝까지 오른쪽으로 가서 한칸 아래로 가는 하드코딩된 최적 policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env, state):\n",
    "    \"\"\"state에 관계없이 policy_s는 4가지 액션이 0.25의 확률을 갖는 array를 리턴한다.\"\"\"\n",
    "    \n",
    "    policy_s = np.ones(env.nA)  / env.nA\n",
    "    \n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(env, state):\n",
    "    \"\"\"하드코딩된 optimal policy\"\"\"\n",
    "    \n",
    "    if state == 36:\n",
    "        return [1.0, 0.0, 0.0, 0.0]\n",
    "    elif state != 35:\n",
    "        return [0.0, 1.0, 0.0, 0.0]\n",
    "    elif state == 35:\n",
    "        return [0.0, 0.0, 1.0, 0.0]\n",
    "    else:\n",
    "        return [0.25, 0.25, 0.25, 0.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성한 policy를 실행하는 시뮬레이션 함수를 만들어본다. \n",
    "\n",
    "시뮬레이션 함수는 첫 스타팅 state 이후 입력받은 policyFunction에 의해 다음 action를 선택하고 실행한다. \n",
    "\n",
    "action을 실행하여 reward와 새로운 next_state를 얻는다.\n",
    "\n",
    "next_state가 종착지이거나, episode의 최대 길이에 도달하는 경우 게임을 종료한다.\n",
    "(이 구현에서는 절벽에 떨어져 -100을 받더라고 게임이 종료된 것으로 간주하지 않는다)\n",
    "\n",
    "그렇지 않은 경우, next_state는 현재 state가 되고 다시 policy에 따른 action을 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " v: -1 | s: 36 | a: LEFT | r: -1 | next s: 36 | done: False\n",
      " v: -101 | s: 36 | a: RIGHT | r: -100 | next s: 36 | done: False\n",
      " v: -102 | s: 36 | a: UP | r: -1 | next s: 36 | done: False\n",
      " v: -103 | s: 24 | a: UP | r: -1 | next s: 24 | done: False\n",
      " v: -104 | s: 12 | a: LEFT | r: -1 | next s: 12 | done: False\n",
      " v: -105 | s: 12 | a: UP | r: -1 | next s: 12 | done: False\n",
      " v: -106 | s: 0 | a: DOWN | r: -1 | next s: 0 | done: False\n",
      " v: -107 | s: 12 | a: RIGHT | r: -1 | next s: 12 | done: False\n",
      " v: -108 | s: 13 | a: LEFT | r: -1 | next s: 13 | done: False\n",
      " v: -109 | s: 12 | a: DOWN | r: -1 | next s: 12 | done: False\n",
      " v: -110 | s: 24 | a: DOWN | r: -1 | next s: 24 | done: False\n",
      " v: -111 | s: 36 | a: LEFT | r: -1 | next s: 36 | done: False\n",
      " v: -211 | s: 36 | a: RIGHT | r: -100 | next s: 36 | done: False\n",
      " v: -212 | s: 36 | a: LEFT | r: -1 | next s: 36 | done: False\n",
      " v: -213 | s: 36 | a: LEFT | r: -1 | next s: 36 | done: False\n",
      " v: -214 | s: 36 | a: LEFT | r: -1 | next s: 36 | done: False\n",
      " v: -314 | s: 36 | a: RIGHT | r: -100 | next s: 36 | done: False\n",
      " v: -414 | s: 36 | a: RIGHT | r: -100 | next s: 36 | done: False\n",
      " v: -415 | s: 36 | a: LEFT | r: -1 | next s: 36 | done: False\n",
      " v: -416 | s: 36 | a: UP | r: -1 | next s: 36 | done: False\n",
      " v: -417 | s: 24 | a: LEFT | r: -1 | next s: 24 | done: False\n",
      " v: -418 | s: 24 | a: RIGHT | r: -1 | next s: 24 | done: False\n",
      " v: -419 | s: 25 | a: LEFT | r: -1 | next s: 25 | done: False\n",
      " v: -420 | s: 24 | a: UP | r: -1 | next s: 24 | done: False\n",
      " v: -421 | s: 12 | a: UP | r: -1 | next s: 12 | done: False\n",
      " v: -422 | s: 0 | a: RIGHT | r: -1 | next s: 0 | done: False\n",
      " v: -423 | s: 1 | a: LEFT | r: -1 | next s: 1 | done: False\n",
      " v: -424 | s: 0 | a: RIGHT | r: -1 | next s: 0 | done: False\n",
      " v: -425 | s: 1 | a: RIGHT | r: -1 | next s: 1 | done: False\n",
      " v: -426 | s: 2 | a: UP | r: -1 | next s: 2 | done: False\n",
      "simulation finished with value = -426.\n"
     ]
    }
   ],
   "source": [
    "def run_simulation(env, policyFunction):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    MAX_EPISODE_LENGTH = 30\n",
    "    \n",
    "    # value를 쌓을 score를 정의한다.\n",
    "    score = 0\n",
    "    \n",
    "    # state를 리셋하고 스타팅 state를 얻는다.\n",
    "    state = env.reset()\n",
    "    \n",
    "    for i in range(MAX_EPISODE_LENGTH):\n",
    "        \n",
    "        # 주어진 state에서 policyFunction을 따르는 action의 확률값을 얻는다.\n",
    "        policy = policyFunction(env, state)\n",
    "        \n",
    "        # action의 확률값을 사용해 action을 선택한다.\n",
    "        action = np.random.choice(np.arange(env.nA), p=policy)\n",
    "    \n",
    "        # action을 실행하여 다음 state, reword, done 여부를 얻는다.\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # 현재 score에 reward를 더해 업데이트한다.\n",
    "        score += reward\n",
    "        \n",
    "        print(\"\\r v: {} | s: {} | a: {} | r: {} | next s: {} | done: {}\".format(score, state, int2action[action], reward, state, done))\n",
    "        sys.stdout.flush()\n",
    "        if done or i == MAX_EPISODE_LENGTH - 1:\n",
    "            print(\"simulation finished with value = {}.\".format(score))\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            state = next_state\n",
    "        \n",
    "run_simulation(env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " v: -1 | s: 36 | a: UP | r: -1 | next s: 36 | done: False\n",
      " v: -2 | s: 24 | a: RIGHT | r: -1 | next s: 24 | done: False\n",
      " v: -3 | s: 25 | a: RIGHT | r: -1 | next s: 25 | done: False\n",
      " v: -4 | s: 26 | a: RIGHT | r: -1 | next s: 26 | done: False\n",
      " v: -5 | s: 27 | a: RIGHT | r: -1 | next s: 27 | done: False\n",
      " v: -6 | s: 28 | a: RIGHT | r: -1 | next s: 28 | done: False\n",
      " v: -7 | s: 29 | a: RIGHT | r: -1 | next s: 29 | done: False\n",
      " v: -8 | s: 30 | a: RIGHT | r: -1 | next s: 30 | done: False\n",
      " v: -9 | s: 31 | a: RIGHT | r: -1 | next s: 31 | done: False\n",
      " v: -10 | s: 32 | a: RIGHT | r: -1 | next s: 32 | done: False\n",
      " v: -11 | s: 33 | a: RIGHT | r: -1 | next s: 33 | done: False\n",
      " v: -12 | s: 34 | a: RIGHT | r: -1 | next s: 34 | done: False\n",
      " v: -13 | s: 35 | a: DOWN | r: -1 | next s: 35 | done: True\n",
      "simulation finished with value = -13.\n"
     ]
    }
   ],
   "source": [
    "run_simulation(env, optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤 policy는 종착지에 도착하지 못했을 뿐더러 value도 크게 낮은 값을 기록한다.  \n",
    "\n",
    "반대로 하드코딩한 optimal policy는 마지막 done이 True이고, 누적 값 역시 -13으로 agent가 얻을 수 있는 최대값을 얻었다.\n",
    "\n",
    "우리의 목표는 agent가 opimal policy를 얻도록 학습하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 무엇이 Optimal Policy일까?\n",
    "\n",
    "위에서 구한 랜덤 policy와 optimal policy는 무슨 차이가 있었을까? optimal policy는 누적 값이 랜덤 policy보다 높았다.\n",
    "\n",
    "누적 값은 각 state에서 얻을 수 있는 reward의 총합이었다. 그럼 누적값이 가장 큰 policy가 optimal policy일까?\n",
    "\n",
    "앞서 강화학습 문제를 Finite Markov Decision Process(Finite MDP)로 정의했다.\n",
    "\n",
    "MDP의 특징 중 하나는 One-Step Dynamics다. 해당 state에서의 의사결정은 다음 state와의 관계속에서 이루어진다.\n",
    "\n",
    "멀리 있는 누적값을 있는 그대로 고려하는 방식이 아니다.\n",
    "\n",
    "즉, 각 state에서 policy를 통해 얻을 수 있는 value를 고려해야 한다.  \n",
    "\n",
    "모든 state에서 다른 policy보다 얻을 수 있는 value가 큰 policy를 optimal policy라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-Value Functions\n",
    "\n",
    "두 policy ($\\pi_1$, $\\pi_2$)가 있다고 하자. 둘 중 어느 policy가 낫다고 할 수 있을까? \n",
    "\n",
    "임의의 state $s$에서 두 policy의 value를 비교해보자. 그리고 모든 state에서 한 policy가 다른 policy보다 value가 크다면, 그 policy는 더 좋다고 할 수 있다.\n",
    "\n",
    "어떤 state $s$에서 어떤 policy $\\pi$가 가지는 value을 구하는 함수를 state-value function이라고 하며 이를 $v_{\\pi}(s)$로 표현한다.\n",
    "\n",
    "이를 수식으로 표현하면,\n",
    "$v_{\\pi}(S) \\doteq E_{\\pi}[G_t | S_t = s]$\n",
    "\n",
    "즉, policy $\\pi$를 따를때 state $s$의 값은 해당 state가 가질 수 있는 value G_t의 기댓값이라 할 수 있다.\n",
    "\n",
    "기대값인 이유는, G_t가 확률변수이기 때문이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equation\n",
    "\n",
    "앞서 value를 계산하는 파트에서 $G_t$를 다음과 같이 정의하였다.\n",
    "\n",
    "$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma R_{t+3} + ..$\n",
    "\n",
    "그런데 $G_t$는 다른 방식으로도 표현할 수 있다.\n",
    "\n",
    "Cliff Walking 문제를 다음과 같이 간단히 바꾸어본다.\n",
    "\n",
    "- state는 3개로 s0 -> s1 -> s2로 agent가 이동한다.\n",
    "- state를 이동할때 reward는 -1을 받는다.\n",
    "- discount rate $\\gamma$는 1로 설정한다.\n",
    "\n",
    "이는 아래 그림과 같다.\n",
    "\n",
    "<img src='assets/simple_pic1.jpg' width=400px>\n",
    "\n",
    "여기서 우리는 마지막 state에 도달했을때 얻는 value를 확실히 안다. 마지막 state에 도달하면 아무 reward를 받지 않고 이후에 받을 reward도 없으므로 이때의 value는 0이다.\n",
    "\n",
    "이 간단한 문제에서 s2는 s1에서부터 이동하므로, s1의 value는 역산하여 구할 수 있다.\n",
    "$v_{\\pi}(s1) = -1 + 0 = -1$\n",
    "\n",
    "<img src='assets/simple_pic2.jpg' width=400px>\n",
    "\n",
    "또 s1의 value를 앎으로써 s0의 value도 알 수 있다. \n",
    "$v_{\\pi}(s1) = -1 + -1 = -2$\n",
    "\n",
    "<img src='assets/simple_pic3.jpg' width=400px>\n",
    "\n",
    "즉 $v_{\\pi}(s)$는 바로 다음에 얻게 될 immediate reward와 다음 state의 value와 같다.\n",
    "\n",
    "$v_{\\pi}(s) = R_{t+1} + v_{\\pi}(s_{t+1})$\n",
    "\n",
    "이를 좀 더 일반화해서 적용해보자.\n",
    "일반적으로 state는 여러 액션을 가질 수 있으므로 다음 reward와 그 다음 state의 value는 확률변수가 된다. 또 다음 state의 value에는 discount가 적용되므로\n",
    "위 식에 $\\gamma$를 곱하고 확률변수의 기댓값 $E$를 씌우면 아래와 같은 식이 된다.\n",
    "\n",
    "$v_{\\pi}(s) = E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s]$\n",
    "\n",
    "이것이 Bellman expectation equation이다.\n",
    "\n",
    "이로서 state가 가지는 value인 $G_t$를 reward와 다음 state의 value로 구할 수 있게 되었다. \n",
    "\n",
    "policy $\\pi$가 deterministic하다면 공식은 다음과 같이 바꿔쓸 수 있다.\n",
    "\n",
    "$v_{\\pi}(s) = \\Sigma_{s^\\prime \\in S^+, r \\in R} p(s^\\prime, r|s, \\pi(s))(r+ \\gamma v_{\\pi}(s^\\prime)$\n",
    "\n",
    "$\\pi(s)$는 state s에 대한 policy에 따른 action이 된다. 즉, $p(s^\\prime, r|s, \\pi(s))$는 state s와 action a가 주어졌을 때 새로운 state $s\\prime$과 reward r이 리턴될 확률이다. 그 다음은 그 action에 대한 immediate reward r과 다음 state $s\\prime$의 discounted된 state value를 더한 보상의 총합이 된다. \n",
    "\n",
    "이를 에피소드의 모든 state $S^+$와 모든 Reward에 대해서 다 합치면 $v_{\\pi}(s)$를 구할 수 있다.\n",
    "\n",
    "만약 $\\pi$가 action이 확률적으로 결정되는 stochastic policy라면 $\\Sigma$에 action이 하나 더 추가되고 뒷단에 $\\pi(a|s)$(state $s$가 주어졌을 때 policy $\\pi$가 action $a$를 선택할 확률)가 추가된다.\n",
    "\n",
    "$v_{\\pi}(s) = \\Sigma_{s^\\prime \\in S^+, r \\in R, a \\in A} \\pi(a|s)p(s^\\prime, r | s, a)(r + \\gamma v_{\\pi}(s^\\prime))$\n",
    "\n",
    "\n",
    "Bellman Equation은 이것 말고도 3개가 더 있는데, 모두 value function이 재귀적인 관계임을 보여준다.\n",
    "\n",
    "앞서 살펴보았던 MDP의 One-Step Dynamics (앞뒤로 위치한 state간에 가지는 관계)를 생각해보면 Bellman Equation과 맞닿아있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Value Functions\n",
    "\n",
    "앞서 각 state에서의 value를 구할 수 있는 state value function에 대해 알아보았다. value는 state에 따라서도 다르지만 각 state에서 취하는 action에 따라서도 달라질 수 있다. 각 state의 action별로 value를 구할 수 있는 function이 action-value function이다.\n",
    "\n",
    "action value function은 state $s$와 action $a$를 인자로 받아 policy $\\pi$를 따를때의 value를 리턴한다. 이를 $q_\\pi$로 표기한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-1568faf775a8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-1568faf775a8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Q = defaultdict(lambda x: )\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q = defaultdict(lambda x: )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<폴리시 계산>\n",
    "\n",
    "State-Value Function은 뭐냐.\n",
    "\n",
    "얘는 $v_\\pi$라고 하는데, 이 함수에 state를 넘기면, 그 policy를 따른다고 했을때, 그 state에서 얻게 되는 value의 값을 리턴하게 된다.\n",
    "\n",
    "즉 $v_{\\pi}(s) \\doteq E_{\\pi}[G_t | S_t = s]$\n",
    "\n",
    "주어진 State(t)가 s일때, 얻을 수 있는 G_t의 기대값을 의미함. 왜 기대값이냐? G_t는 확률변수이기 때문에. (변할 수 있기 때문에 이를 기대값으로 표현)\n",
    "\n",
    "$v_{\\pi}(s)$ = value of state $s$ under policy $\\pi$라고 한다.\n",
    "\n",
    "G_t를 어떻게 풀어야 하지?\n",
    "\n",
    "어떤 스테이트에서의 value는 그 다음 reward + discounted된 그 다음 스테이트의 value임 \n",
    "\n",
    "\n",
    "## Bellman Equations\n",
    "\n",
    "Bellman expectation equation for $v_{\\pi}$\n",
    "\n",
    "왜 나오지 이게? \n",
    "$v_{\\pi}(s) = E[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) | S_t = s]$\n",
    "\n",
    "$G_t$를 $R_{t+1} + \\gamma v_{\\pi}(S_{t+1})$로 다시 쓴거야.\n",
    "\n",
    "## Optimality\n",
    "\n",
    "$\\pi^*$는 최적의 폴리시. 어떤 스테이트에서든지 이 폴리시를 따르면 그 스테이트 밸류 펑션이 다른 어떤 폴리시의 그것보다 같거나 크다. 그러니까 이게 최고. 이 최적 폴리시는 유니크하지는 않아도 반드시 존재함.\n",
    "\n",
    "그리고 모든 최적의 폴리시는 같은 스테이트 밸류 펑션을 가진다. 왜냐면, 복수라고 같아야 하니까. 그걸 $v^*$라고 하고 optimal state-value function이라고 한다.\n",
    "\n",
    "## Action-Value Functions\n",
    "\n",
    "value는 state뿐만 아니라 state-action 페어로도 정의할 수 있음.\n",
    "이를 $q_{\\pi}$라고 함.\n",
    "\n",
    "$q_{\\pi} \\doteq E_{\\pi}[G_t | S_t = s, A_t = a]$\n",
    "폴리시 $\\pi$를 따를 때 state s에서 action a를 쓸때 얻는 밸류를 리턴하는 펑션\n",
    "\n",
    "마찬가지로 모든 최적 폴리시는 같은 옵티멀 q함수를 가진다. optimal action-value function\n",
    "\n",
    "## Optimal Polices\n",
    "그래서 agent가 최적의 action-value function을 결정하게 되면, 그다음에는 바로 최적 폴리시를 찾을 수 있어. $\\pi^*(s) = argmax_{a \\in A(s)} q^*(s, a)$\n",
    "\n",
    "최적 폴리시에 s를 넣으면 나오는 결과는 모냐면, s에서 취할 수 있는 액션 중에, 옵티멀함수 q에 s와 a를 넣었을 때 나올 수 있는 값을 최대화시키는 액션. 즉.. 모냐면 해당 상황에서 취할 수 있는 액션 중에, 가장 큰 밸류를 얻을 수 있는 액션을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 value를 설명할 때 value는 앞으로 얻게 될 reward의 총 합이라고 했다. 또 미래 시점의 reward는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_values(V):\n",
    "    # reshape the state-value function\n",
    "    V = np.reshape(V, env.shape)\n",
    "    # plot the state-value function\n",
    "    fig, ax = plt.subplots(figsize=env.shape[::-1])\n",
    "    im = sns.heatmap(V_opt, annot=True, cmap='RdBu', linewidths=1)\n",
    "    plt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "    plt.title('State-Value Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_opt = np.zeros((4, 12))\n",
    "plot_values(V_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
